{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adcdc885",
   "metadata": {},
   "source": [
    "# Assignment - 2: Data Representation and Point Cloud Operations\n",
    "\n",
    "Team Name: Robo-Knights\n",
    "\n",
    "Roll number:  2019111007, 2019112002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fabf26",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "- Code must be written in Python in Jupyter Notebooks. We highly recommend using anaconda distribution or at the minimum, virtual environments for this assignment.\n",
    "- Save all your results in ```results/<question_number>/<sub_topic_number>/```\n",
    "- The **References** section provides you with important resources to solve the assignment.\n",
    "- Make sure your code is modular since you may need to reuse parts for future assignments.\n",
    "- Answer the descriptive questions in your own words with context & clarity. Do not copy answers from online resources or lecture notes.\n",
    "- The **deadline** for this assignment is on 26/09/2021 at 11:55pm. Please note that there will be no extensions.\n",
    "- Plagiarism is **strictly prohibited**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70ba8bb",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "1. Make sure your code runs without any errors after reinitializing the kernel and removing all saved variables.\n",
    "2. After completing your code and saving your results, zip the folder with name as ``Team_<team_name>_MR2021_Assignment_<assignment_number>.zip``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f8d1fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-09-28 20:55:27,548 - utils - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install opencv-python\n",
    "# !pip install python-utils\n",
    "# !pip show requests\n",
    "# !pip install pynput\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "import os\n",
    "import glob\n",
    "from open3d import *\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "import math\n",
    "from numpy.linalg import inv\n",
    "from pynput.keyboard import Key, Listener\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7470315b",
   "metadata": {},
   "source": [
    "# Introduction to types of Transformations and Homogeneous coordinates\n",
    "\n",
    "In robotics applications, it is inevitable to keep track of the frames of multiple objects/worlds. These frames can be transformations from one coordinate frame to the other. **Homogeneous coordinates** help in keeping track of various coordinate frames and allow performing composition of various transforms. We will first try to understand between types of transformations and their invariant properties.\n",
    "1. What is the difference between Affine, Similarity, and Euclidean transform? What are the invariant properities of each type of transform?\n",
    "2. Watch this [video](https://www.youtube.com/watch?v=PvEl63t-opM) to briefly understand homogeneous coordinates. What are points at infinity? What type of transformation can you apply to transform a point from infinity to a point that is not at infinity? \n",
    "3. Using homogeneous coordinates we can represent different types of transformation as point transforms vs. frame transforms. Concatenation of transforms (whether you post multiply transformation matrices or pre-multiply transformation matrices) depends on the problem and how you are viewing it. Try to understand the difference between frame vs. point transformations from this [video](https://youtu.be/Za7Sdegf8m8?t=1834). Let's assume that our camera and world frames are coinciding with each other. We need to estimate the camera to world **frame** transformation matrix after applying the transformations defined below in terms of $T_i$.We apply **frame** transform to move the camera in the world in the following order:\n",
    "    1. $T_1$ from the camera coordinate frame.\n",
    "    2. $T_2$ from the world coordinate frame.\n",
    "    3. $T_3$ from the world coordinate frame.\n",
    "    4. $T_4$ from the camera coordinate frame.\n",
    "    5. $T_5$ from the camera coordinate frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb963da",
   "metadata": {},
   "source": [
    "1. \n",
    "    a. Affine transform preserves lines (meaning that it sends points to points, lines to lines, planes to planes, and so on) and parallelism i.e., all points lying on a line initially still lie on a line after transformation and it preserves ratios and distances (along parallel directions) but not necessarily distances and euler angles. Eg: Reflection, rotation, shear, translation, etc.  \n",
    "    \n",
    "    b. Euclidean transform preserves distances and angles. It can only change orientation and position of a figure in space. The size of the figure remains same. Eg: Rotation, translation, reflection. Any Euclidean transformation is an affine transformation.\n",
    "    \n",
    "    c. Let C be a square nonsingular matrix having the same order as the matrix A. We say that the matrices A and $C^{‚àí1}AC$ are similar, and the transformation from A to $C^{‚àí1}AC$ is called a similarity transformation. Moreover, we say that the two matrices are unitarily similar if C is unitary. Similarity transformations transform objects in space to similar objects.The determinant of the similarity transformation of a matrix is equal to the determinant of the original matrix. Two similar matrices share the same spectrum and the same characteristic polynomial.Properties: \n",
    "    1. Distinct points are mapped to distinct points. \n",
    "    2. Each point ùëÉ‚Ä≤ in the plane has a pre-image. \n",
    "    3. A similarity transformation sends lines to lines, rays to rays, line segments to line segments, and parallel lines to parallel lines. \n",
    "    4. A similarity transformation sends angles to angles of equal measure. \n",
    "    5. A similarity transformation maps a circle of radius ùëÖ to a circle of radius ùëüùëÖ, where ùëü is the scale factor of the similarity transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f02ce1d",
   "metadata": {},
   "source": [
    "2. Points at infinity are the the points that are at a very large distance from camera. Informally defined as the limit of a point that moves in that direction away from the origin. point at infinity corresponding to the direction of the line ax + by = 0. Points at infinity can be expressed in homogeneous by setting the last coordinate to 0. Points at infinity expressed in homogeneous coordinates preserves the direction of the point. To convert from point at infinity to a finite point,we set the last coordinate to a finite value. A point at infinity can be shown to transform to a finite point using perspective projection.  \n",
    "One possible transformation matrix is: [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, r ,1]], point at infinity = [x, y, z, 0]. Here r is a constant and r and z are non-zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986ddd79",
   "metadata": {},
   "source": [
    "3. Assuming $T_i$ is the initial transformation matrix for a camera.  Camera to world frame transformation matrix: $(T_3T_2T_iT_1T_4T_5)^{-1}$\n",
    "    1. $T_1$ from the camera coordinate frame: Since the tranformation is done in current frame we post multiply. T = $T_i$$T_1$\n",
    "    2. $T_2$ from the world coordinate frame: Since the tranformation is done in world frame we pre multiply.  T = $T_2$$T_i$$T_1$\n",
    "    3. $T_3$ from the world coordinate frame: Since the tranformation is done in world frame we pre multiply.  T = $T_3$$T_2$$T_i$$T_1$\n",
    "    4. $T_4$ from the camera coordinate frame: Since the tranformation is done in current frame we post multiply. T = $T_3$$T_2$$T_i$$T_1$$T_4$\n",
    "    5. $T_5$ from the camera coordinate frame: Since the tranformation is done in current frame we post multiply. T = $T_3$$T_2$$T_i$$T_1$$T_4$$T_5$\n",
    "\n",
    "But since we need transformation from the camera to world frame we take inverse of T. Therefore final transformation matrix is <br/>\n",
    "$(T_3T_2T_iT_1T_4T_5)^{-1}$ =\n",
    "$T_5^{-1}$*$T_4^{-1}$*$T_1^{-1}$*$T_i^{-1}$*$T_2^{-1}$$T_3^{-1}$ \n",
    "\n",
    "If $T_i$ is not the initial transformation matrix for a camera, the final transformation matrix is : $T_5^{-1}$*$T_4^{-1}$*$T_1^{-1}$*$T_2^{-1}$*$T_3^{-1}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dadfa0",
   "metadata": {},
   "source": [
    "# Visualise the Data\n",
    "\n",
    "Point clouds are a collection of points that represent a 3D shape or feature. Each point has its own set of X, Y and Z coordinates and in some cases additional attributes. A popular way to obtain this is by photogrammetry, though here we will use LiDAR data.\n",
    "\n",
    "LiDAR is a remote sensing process which collects measurements used to create 3D models and maps of objects and environments. Using ultraviolet, visible, or near-infrared light, LiDAR gauges spatial relationships and shapes by measuring the time it takes for signals to bounce off objects and return to the scanner.\n",
    "\n",
    "1. Download the data from [here](https://iiitaphyd-my.sharepoint.com/:f:/g/personal/venkata_surya_students_iiit_ac_in/EnYAMaTVIhJItzKYqtahE30BRKB6p6UfHN3TyJzvo6Mw0g?e=PegWds). It contains the LIDAR sensor output and odometry information per frame.\n",
    "\n",
    "    The .bin files contain the 3D point cloud captured by the LIDAR in this format - x, y, z, and reflectance. \n",
    "\n",
    "    The odometry information is given in the `odometry.txt` file, which is a 12 element vector. Reshape each of the first 77 rows to a 3x4 matrix to obtain the pose.\n",
    "    \n",
    "\n",
    "2. Obtain the point cloud from this and visualise for 1-2 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee682294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data from odometry.txt\n",
    "# 1\n",
    "def readData(filename):\n",
    "    data = np.loadtxt(filename)\n",
    "    return data \n",
    "\n",
    "# reading the bin files\n",
    "def readPointCloud(filename):\n",
    "    pcl = np.fromfile(filename, dtype=np.float32,count=-1)\n",
    "    pcl = pcl.reshape([-1,4])\n",
    "    return pcl\n",
    "\n",
    "# Obtaining the transformation matrix\n",
    "def getTransformationMat(odometry ,i):\n",
    "    T = odometry[i][:]\n",
    "    T = np.reshape(T, (3,4))\n",
    "    b = np.array([0,0,0,1])\n",
    "    T = np.vstack ((T,b)) \n",
    "    return T\n",
    "\n",
    "# Displaying point cloud from the given list\n",
    "def displayPointCloud(pc):\n",
    "    pcd_dis = o3d.geometry.PointCloud()  \n",
    "    len_ = len(pc)\n",
    "    for nn in range(len_):\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        each_pc = pc[nn].T[:, :3]\n",
    "        pcd.points = o3d.utility.Vector3dVector(each_pc)\n",
    "        pcd_dis += pcd\n",
    "    pcd_dis = pcd_dis.voxel_down_sample(voxel_size = 1)\n",
    "    o3d.visualization.draw_geometries([pcd_dis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc6f8083",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_lidar = './data/LiDAR/'\n",
    "directory_odometry = './data/odometry.txt'\n",
    "\n",
    "odometry_data = readData(directory_odometry)\n",
    "odometry_data = odometry_data[:77][:]\n",
    "\n",
    "# Saving the lidar file names \n",
    "lidarFiles = [f for f in listdir(directory_lidar) if isfile(join(directory_lidar, f))]\n",
    "\n",
    "lidarFiles = sorted(lidarFiles)\n",
    "\n",
    "pc  = []\n",
    "pc_ = []\n",
    "pc__ = []\n",
    "# 2\n",
    "# Iterating through the 77 bin files and saving the point for displaying\n",
    "for f in lidarFiles: \n",
    "    f_data = readPointCloud(directory_lidar + f)\n",
    "    x = np.ones(f_data.shape[0])\n",
    "    # Replacing the reflectance values with 1 as they are not needed.\n",
    "    f_data[:,3] = x \n",
    "    f_data = f_data.T\n",
    "    pc.append(f_data)\n",
    "    \n",
    "pc_.append(pc[0]) \n",
    "pc__.append(pc[33])\n",
    "# Visualizing 1th frame\n",
    "displayPointCloud(pc_)\n",
    "# Visualizing 34th frame\n",
    "displayPointCloud(pc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb19901",
   "metadata": {},
   "source": [
    "# Transform \n",
    "\n",
    "The point cloud obtained is with respect to the LiDAR frame. The poses however, are in the camera frame. If we want to combine the point clouds from various frames, we need to bring them to the camera frame. \n",
    "\n",
    "1. Refer to the image below and apply the required transformation to the point cloud. \n",
    "\n",
    "2. Then, register all point clouds into a common reference frame and visualise it (Open3D). It is helpful to use homogeneous coordinates to keep track of the different frames.\n",
    "\n",
    "3. Write a function to transform the registered point cloud from the world to the $i^{th}$ camera frame, wherein $i$ is the input to the function.\n",
    "\n",
    "4. \\[Bonus\\] Move around in the registered point cloud using arrow keys like you would do in a game. For this you will have to regularly transform the entire registered world to your current camera frame and visualize repeatedly. You may choose to avoid visualizing points that are behind the camera in this case as they are not visible from the scene. You may also visualize points at a max depth to make the process easier.\n",
    "\n",
    "![](./img/transform.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7894f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# T_lidar_camera = [[Xl.Xc, Yl.Xc, Zl.Xc, 0], [Xl.Yc, Yl.Yc, Zl.Yc,0], [Xl.Zc, Yl.Zc, Zl.Zc,0], [0,0,0,1]]\n",
    "T_lidar_camera = np.array([[0,-1,0,0],[0,0,-1,0],[1,0,0,0],[0,0,0,1]]) \n",
    "# T_camera_world = inv(T_lidar_camera)\n",
    "T_camera_world = np.array([[0,0,1,0],[-1,0,0,0],[0,-1,0,0],[0,0,0,1]])\n",
    "\n",
    "points_in_world = []\n",
    "point_in_camera = []\n",
    "points_in_world_world = []\n",
    "# Transform the point cloud from the Lidar‚Äôs frame to the Camera‚Äôs frame\n",
    "for i in range(77): \n",
    "    point_in_camera.append(T_lidar_camera@pc[i])\n",
    "    \n",
    "# 2\n",
    "# Transform the point cloud from the Camera‚Äôs frame to the Common frame \n",
    "# Apply the global transformation obtained from the .txt file.\n",
    "# World frame is considered assuming the given transformation takes the points form camera frame to the world\n",
    "for i in range(77):\n",
    "#   Common frame\n",
    "    points_in_world.append(T_camera_world@(getTransformationMat(odometry_data ,i)@point_in_camera[i])) \n",
    "#   World frame\n",
    "    points_in_world_world.append((getTransformationMat(odometry_data ,i)@point_in_camera[i])) \n",
    "    \n",
    "displayPointCloud(points_in_world)\n",
    "# displayPointCloud(points_in_world_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e1350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "# Transforming from common frame to ith camera frame\n",
    "def transformation_ith_cam(points, cam_num):\n",
    "    points_given_cam = []\n",
    "    for i in range(77):\n",
    "        points_given_cam.append(inv(T_camera_world@(getTransformationMat(odometry_data ,cam_num)))@points[i])\n",
    "#     displaying the transformed point cloud    \n",
    "    displayPointCloud(points_given_cam)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7630adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing form the ith_frame the registered point cloud\n",
    "ith_frame = 4\n",
    "transformation_ith_cam(points_in_world, ith_frame - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "866495d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Bonus Obtaining the transformed point cloud in the required camera frame\n",
    "def transform_to_given_camera(points, cam_num):\n",
    "    point_in_reqframe = []\n",
    "    for i in range(77):\n",
    "        point_in_reqframe.append(inv(T_camera_world@(getTransformationMat(odometry_data ,cam_num)))@points[i])\n",
    "    pcd_dis = o3d.geometry.PointCloud()  \n",
    "    len_ = len(point_in_reqframe)\n",
    "    for nn in range(len_):\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        each_pc = point_in_reqframe[nn].T[:, :3]\n",
    "        pcd.points = o3d.utility.Vector3dVector(each_pc)\n",
    "        pcd_dis += pcd\n",
    "    pcd_dis = pcd_dis.voxel_down_sample(voxel_size = 1)\n",
    "    return pcd_dis   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2347a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_draw_geometry_with_key_callback(pcd):\n",
    "    \n",
    "    \n",
    "    def move_to_next_frame(vis):\n",
    "        global current_frame\n",
    "        global current_trans \n",
    "        current_frame = (current_frame + 1)%77\n",
    "#       Obtaining the required transformation matrix required to transform the points from common frame to current camera frame \n",
    "        mat = inv(T_camera_world@getTransformationMat(odometry_data ,current_frame))\n",
    "#       Taking the current point first to common frame and then to the required camera frame\n",
    "        net_trans = mat@current_trans\n",
    "#       Setting the view control to only display the points in front of the camera\n",
    "        ctr = vis.get_view_control()\n",
    "        ctr.change_field_of_view(step=90)\n",
    "        ctr.set_front([ -0.015120344908731674, 0.044412925228585441, -0.99889882733061675 ])\n",
    "        ctr.set_lookat([ -9.5100493742385073, -3.7686547525759422, 26.032425470440081 ])\n",
    "        ctr.set_up([ -0.0361140904310902, -0.99838545923018884, -0.043843440445063948 ])\n",
    "        ctr.set_zoom(0.25999999999999995)\n",
    "        pcd.transform(net_trans)\n",
    "        vis.update_geometry(pcd)\n",
    "        vis.poll_events()\n",
    "        vis.update_renderer()\n",
    "        current_trans = inv(mat)\n",
    "        return False\n",
    "    \n",
    "    def move_to_previous_frame(vis):\n",
    "        global current_frame\n",
    "        global current_trans \n",
    "        current_frame = (current_frame - 1)%77\n",
    "        mat = inv(T_camera_world@getTransformationMat(odometry_data ,current_frame))\n",
    "        \n",
    "        net_trans = mat@current_trans\n",
    "        ctr = vis.get_view_control()\n",
    "        ctr.change_field_of_view(step=90)\n",
    "        ctr.set_front([ -0.015120344908731674, 0.044412925228585441, -0.99889882733061675 ])\n",
    "        ctr.set_lookat([ -9.5100493742385073, -3.7686547525759422, 26.032425470440081 ])\n",
    "        ctr.set_up([ -0.0361140904310902, -0.99838545923018884, -0.043843440445063948 ])\n",
    "        ctr.set_zoom(0.25999999999999995)\n",
    "        pcd.transform(net_trans)\n",
    "        vis.update_geometry(pcd)\n",
    "        vis.poll_events()\n",
    "        vis.update_renderer()\n",
    "        current_trans = inv(mat)\n",
    "        return False\n",
    "    \n",
    "    mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size = 30, origin = [-9.5,0,-20])\n",
    "    \n",
    "    key_to_callback = {}\n",
    "    key_to_callback[ord(\"D\")] = move_to_next_frame\n",
    "    key_to_callback[ord(\"A\")] = move_to_previous_frame\n",
    "    o3d.visualization.draw_geometries_with_key_callbacks([pcd], key_to_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6b368c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Press 'D' to move to next frame\n",
      "   Press 'A' to move to previous frame\n"
     ]
    }
   ],
   "source": [
    "# Enter the starting frame \n",
    "current_frame = 50\n",
    "pointCloud_obtained = transform_to_given_camera(points_in_world, current_frame)\n",
    "current_trans = T_camera_world@getTransformationMat(odometry_data , current_frame)\n",
    "print(\"   Press 'D' to move to next frame\")\n",
    "print(\"   Press 'A' to move to previous frame\")\n",
    "custom_draw_geometry_with_key_callback(pointCloud_obtained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb054ba",
   "metadata": {},
   "source": [
    "Find the link for the recorded animated video: https://drive.google.com/drive/folders/1jBnCNjmx7yScjUowzy7RjjBTHZ5y2Awv?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87734058",
   "metadata": {},
   "source": [
    "# Occupancy Grid\n",
    "Occupancy grid maps are discrete fine grain grid maps. These maps can be either 2-D or 3-D. Each cell in the occupancy grid map contains information on the physical objects present in the corresponding space. Since these maps shed light on what parts of the environment are occupied, and what is not, they are really useful for path planning and navigation.\n",
    "\n",
    "Occupancy grid maps are probabilistic in nature due to noisy measurements. Each cell can have three states: Occupied, unoccupied, and unknown. For the purpose of this assignment, you can ignore the unknown and work in a binary setting where 1 is occupied and 0 is unoccupied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063046a2",
   "metadata": {},
   "source": [
    "1. The task here is to create an occupancy map for each LiDAR scan. You do not need to apply bayesian update rules here, just keep it simple. \n",
    "\n",
    "2. Now, using the *registered* point cloud, generate occupancy maps for each frame. What difference do you expect to see between the two methods?\n",
    "\n",
    "You can mark a cell as occupied based on a threshold of how many different z values are there for a particular (x,y) cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cefb054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "def create_occupancy_grid_LiDAR_i(points, threshold, occ):\n",
    "    len_ = len(points)\n",
    "    dimensions = np.zeros((len_,4), dtype='int64')\n",
    "    for i in range(len_): \n",
    "#       Since the occupancy grid is basically an image with discrete integer indices, we will round off the Lidar‚Äôs measurements to integer values.\n",
    "        points_rounded = points[i].astype('int64')\n",
    "        points_rounded = points_rounded.T[:, :3]\n",
    "        \n",
    "#       Next, we find the max and min values for the x and y coordinates in order to determine the dimensions of the occupancy grid.\n",
    "        dimensions[i,0] = np.amax(points_rounded[:,0])\n",
    "        dimensions[i,1] = np.amin(points_rounded[:,0])\n",
    "        dimensions[i,2] = np.amax(points_rounded[:,1])\n",
    "        dimensions[i,3] = np.amin(points_rounded[:,1])\n",
    "        occupancy_grid = np.zeros((abs(dimensions[i,0] - dimensions[i,1]) + 1, abs(dimensions[i,2] - dimensions[i,3]) + 1))\n",
    "        \n",
    "#       Since we want to generate different values of z for a particular (x,y), we need to remove the duplicate coordinates.\n",
    "        points_rounded = np.unique(points_rounded, axis=0)\n",
    "    \n",
    "#    After removing duplicates count the occurrence of points which fall in the same cell but counting the different values of z for a pair in the array\n",
    "        for nn in range(points_rounded[:,1].size):\n",
    "            occupancy_grid[points_rounded[nn,0] - dimensions[i,1], points_rounded[nn,1] - dimensions[i,3]] += 1\n",
    "        if occ == 1:\n",
    "#             free\n",
    "#       For obtaining the free map we normalize so as to set a proper threshold that would give us the free area in the map.\n",
    "            occupancy_grid = occupancy_grid/points_rounded[:,1].size\n",
    "            image_path = str(i)+\"_free.png\"\n",
    "    \n",
    "        else:\n",
    "#             occupied\n",
    "            image_path = str(i)+\"_occupied.png\"\n",
    "    \n",
    "#       After getting the count of multiple z we mark the cell as an obstacle only if the count of z values for that cell is greater than threshold else it will be considered as empty.\n",
    "        occupancy_grid = (occupancy_grid > threshold)\n",
    "#       The results are saved using the PIL library.\n",
    "        save_to_path = './results/Question_4_1/'\n",
    "        path_final = save_to_path + image_path\n",
    "        np.asarray(occupancy_grid)\n",
    "        occupancy_grid = Image.fromarray(occupancy_grid)\n",
    "        occupancy_grid.save(path_final, 'PNG')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fa2524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# occupied\n",
    "create_occupancy_grid_LiDAR_i(pc, 1, 0)\n",
    "# free\n",
    "create_occupancy_grid_LiDAR_i(pc, 0.00003, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8514c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "def create_occupancy_grid_LiDAR_ii(points, threshold, occ):\n",
    "    # Finding the dimension of the grid\n",
    "    all_points = []\n",
    "    dimensions = np.zeros((1,4), dtype='int64')\n",
    "    len_ = len(points)\n",
    "    \n",
    "    for i in range(len_): \n",
    "#       Since the occupancy grid is basically an image with discrete integer indices, we will round off the Lidar‚Äôs measurements to integer values.\n",
    "        points_rounded = points[i].astype('int64')\n",
    "        points_rounded = points_rounded.T[:, :3]\n",
    "#       Since we want to generate different values of z for a particular (x,y), we need to remove the duplicate coordinates.\n",
    "        points_rounded = np.unique(points_rounded, axis=0)\n",
    "        all_points.append(points_rounded)\n",
    "    \n",
    "    final_array = all_points[0]\n",
    "    \n",
    "    for i in range(len_ - 1): \n",
    "        final_array = np.concatenate((final_array, all_points[i+1]), axis=0)  \n",
    "        \n",
    "#   Since we want to generate different values of z for a particular (x,y), we need to remove the duplicate coordinates.    \n",
    "    final_array_rounded = np.unique(final_array, axis=0)\n",
    "    \n",
    "#   Next, we find the max and min values for the x and y coordinates in order to determine the dimensions of the occupancy grid.    \n",
    "    dimensions[0,0] = np.amax(final_array_rounded[:,0])\n",
    "    dimensions[0,1] = np.amin(final_array_rounded[:,0])\n",
    "    dimensions[0,2] = np.amax(final_array_rounded[:,1])\n",
    "    dimensions[0,3] = np.amin(final_array_rounded[:,1])\n",
    "    occupancy_grid = np.zeros((abs(dimensions[0,0] - dimensions[0,1]) + 1, abs(dimensions[0,2] - dimensions[0,3]) + 1))\n",
    "    \n",
    "#    After removing duplicates count the occurrence of points which fall in the same cell but counting the different values of z for a pair in the array\n",
    "    for nn in range(final_array_rounded[:,1].size):\n",
    "        occupancy_grid[final_array_rounded[nn,0] - dimensions[0,1], final_array_rounded[nn,1] - dimensions[0,3]] += 1\n",
    "\n",
    "#   After getting the count of multiple z we mark the cell as an obstacle only if the count of z values for that cell is greater than threshold else it will be considered as empty.    \n",
    "    if occ == 1:\n",
    "#       For obtaining the free map we normalize so as to set a proper threshold that would give us the free area in the map.\n",
    "        occupancy_grid = occupancy_grid/final_array_rounded[:,1].size\n",
    "        image_path = \"Final_4_ii_free\"+\".png\"    \n",
    "    else:\n",
    "        image_path = \"Final_4_ii_occupied\"+\".png\"\n",
    "        \n",
    "    occupancy_grid = (occupancy_grid > threshold)\n",
    "    \n",
    "#   The results are saved using the PIL library.    \n",
    "    save_to_path = './results/Question_4_2/'\n",
    "#   image_path = \"Final_4_ii\"+\".png\"\n",
    "    path_final = save_to_path + image_path\n",
    "    np.asarray(occupancy_grid)\n",
    "    occupancy_grid = Image.fromarray(occupancy_grid)\n",
    "    occupancy_grid.save(path_final, 'PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a38acbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# occupied\n",
    "create_occupancy_grid_LiDAR_ii(points_in_world, 2, 0)\n",
    "# free \n",
    "create_occupancy_grid_LiDAR_ii(points_in_world, 0.00003, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314c709",
   "metadata": {},
   "source": [
    "Differences Observed:\n",
    "1. In (1) the array we took is only points corresponding to the particular timestamp calculation whereas in (2) we took the array of points which includes all the 77 time stamps together.\n",
    "2. The visualized point cloud in case (2) is more dense than in case (1) since we visualize the all the 77 frames in a single window in case (2) and only one frame is visualized in part (1). The threshold taken in case (1) is 1 whereas (2) we will increase the threshold for better estimate of the obstacle at that particular position. This is because since we are joining points form different point clouds the probability of error increases. So to get a better map from noisy measurements we increase the threshold value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
